# ğŸ CricketVerse

![Build Status](https://img.shields.io/github/actions/workflow/status/asrafaligit/CricketVerse/scraper.yml?branch=main)
![Repo Size](https://img.shields.io/github/repo-size/asrafaligit/CricketVerse)
![Last Commit](https://img.shields.io/github/last-commit/asrafaligit/CricketVerse)
![Issues](https://img.shields.io/github/issues/asrafaligit/CricketVerse)
![License](https://img.shields.io/badge/license-MIT-green)

---

## ğŸ“Œ Overview

**CricketVerse** is a full-stack live cricket match tracking and prediction platform.

- ğŸ” Scrapes **live scores** from ESPN CricInfo.
- â˜ï¸ Stores **match data** in **MongoDB Atlas**.
- ğŸ”„ Fetches **weather data** for match venues.
- ğŸ§© Runs **ML models** to predict match outcomes & toss decisions.
- âš™ï¸ Provides REST APIs for your frontend.
- ğŸŒ Deployed with **Render**, **GitHub Actions** for automation.

---

## ğŸ“‚ Project Structure

CricketVerse/
â”œâ”€â”€ backend/ # Express server (Node.js)
â”‚ â”œâ”€â”€ backend.js
â”‚ â”œâ”€â”€ .env # Environment variables (Mongo URI, API keys)
â”‚ â”œâ”€â”€ package.json
â”œâ”€â”€ Scraping/ # Python scraper (BeautifulSoup, Requests)
â”‚ â”œâ”€â”€ Scraper.py
â”œâ”€â”€ .github/workflows/ # GitHub Actions for cron scraping
â”‚ â”œâ”€â”€ scraper.yml
â”œâ”€â”€ README.md # This file!

markdown
Copy
Edit

---

## âš™ï¸ Tech Stack

- **Frontend:** React (separate repo or `/frontend` if included)
- **Backend:** Node.js + Express
- **Database:** MongoDB Atlas
- **Scraper:** Python (Requests, BeautifulSoup)
- **Automation:** GitHub Actions
- **Deployment:** Render

---

## ğŸš€ How It Works

1ï¸âƒ£ **Python Scraper**

- Runs every **5 mins** (GitHub Actions).
- Fetches live matches, scorecards, venues & dates.
- Sends data to backend API `/save-data`.

2ï¸âƒ£ **Express Backend**

- Receives & stores match data.
- Calls **Visual Crossing Weather API** if weather is missing.
- Provides `/get-data` & `/get-match-by-matchid` endpoints.

3ï¸âƒ£ **Frontend**

- Calls backend APIs.
- Displays live scores, win predictions, toss advisor.

---

## ğŸ”‘ Environment Variables

You **must** set these:

| Key                       | Description                     |
| ------------------------- | ------------------------------- |
| `MONGO_URI`               | MongoDB Atlas connection string |
| `VISUAL_CROSSING_API_KEY` | Weather API key                 |
| `REACT_APP_API_URL`       | Backend base URL (for scraper)  |

**Where to add:**

- `.env` in `/backend`
- GitHub â†’ `Settings` â†’ `Secrets` â†’ `Actions` â†’ add `REACT_APP_API_URL`

---

## âš™ï¸ Running Locally

**Backend:**

```bash
cd backend
npm install
node backend.js
Scraper (manual):

bash
Copy
Edit
cd Scraping
pip install requests beautifulsoup4 fake-useragent python-dotenv
python Scraper.py
âš¡ GitHub Actions Cron
Runs automatically every 5 mins.

File: .github/workflows/scraper.yml

Also can be triggered manually under Actions tab.

ğŸ§  Predictions
2 ML models:

Win probability

Toss decision

(These should be deployed via backend routes like /predict-win or /predict-toss. Not detailed here.)

ğŸŒ Deployment
Backend: Render

Frontend: (same or different host)

Database: MongoDB Atlas

ğŸ“œ License
MIT License

ğŸ™Œ Author
Asraf Ali

â­ï¸ Show Support
If you find this useful:

âœ… Star this repo
âœ… Fork & contribute
âœ… Open issues & pull requests

ğŸ“£ Feedback
Feel free to file an issue or connect with me for improvements.
```
